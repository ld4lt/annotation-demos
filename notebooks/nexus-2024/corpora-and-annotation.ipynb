{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYzPl6eMDlTI"
      },
      "source": [
        "# LLOD demo corpus\n",
        "\n",
        "This notebook uses Python and a number of common Unix tools to\n",
        "- retrieve PDFs from the web,\n",
        "- extract their text,\n",
        "- perform syntactic analysis,\n",
        "- convert to RDF, and\n",
        "- download the results\n",
        "\n",
        "You can press `<CTRL>+F9` (or select `Kernel`|`Run all` from the menu) to run the full pipeline.\n",
        "\n",
        "> *Note*: The notebook is designed to run in [Google Colaboratory](https://colab.research.google.com/) and uses its extensions for downloading output files. If you run it on your local [Jupyter Notebook](https://jupyter.org/) installation, you can comment this out, and instead, access the files directly on your local harddrive.\n",
        "\n",
        "> *Note*: As a number of pre-requisites need to be installed and because of hardware limitations on Google Colab, running this script will take several minutes. In particular, the parser seems to be slow. Don't panic, just give it some slack ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjAF3v6iaFz8",
        "outputId": "2386497e-1e31-44b7-e488-4fb32af77de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing text extraction scripts (GhostScript)\n",
            "================================================\n",
            "done\n",
            "\n",
            "installing parser (spaCy)\n",
            "=========================\n",
            "done\n",
            "\n",
            "installing FINTAN\n",
            "=================\n",
            "--2024-03-29 12:07:59--  https://github.com/acoli-repo/fintan-backend/releases/download/fintan-backend-release-v1.0.0/fintan.jar\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/292584141/2d8d3ce7-825c-44d5-9b77-b1d10bdaacea?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240329%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240329T120800Z&X-Amz-Expires=300&X-Amz-Signature=6526bb6039d992eef221606228a83fb989698f0e25731210f4133a671df87ba6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=292584141&response-content-disposition=attachment%3B%20filename%3Dfintan.jar&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-03-29 12:08:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/292584141/2d8d3ce7-825c-44d5-9b77-b1d10bdaacea?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240329%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240329T120800Z&X-Amz-Expires=300&X-Amz-Signature=6526bb6039d992eef221606228a83fb989698f0e25731210f4133a671df87ba6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=292584141&response-content-disposition=attachment%3B%20filename%3Dfintan.jar&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27936001 (27M) [application/octet-stream]\n",
            "Saving to: ‘fintan.jar’\n",
            "\n",
            "fintan.jar          100%[===================>]  26.64M  48.5MB/s    in 0.5s    \n",
            "\n",
            "2024-03-29 12:08:01 (48.5 MB/s) - ‘fintan.jar’ saved [27936001/27936001]\n",
            "\n",
            "--2024-03-29 12:08:01--  https://github.com/acoli-repo/fintan-backend/releases/download/fintan-backend-release-v1.0.0/run.sh\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/292584141/ba8c6091-57ba-40fd-aa44-da9906d56075?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240329%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240329T120802Z&X-Amz-Expires=300&X-Amz-Signature=3b8e513ffc0896f923a50c511ad3b2f9a41ca97997daf7cc7af03e257783bc3f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=292584141&response-content-disposition=attachment%3B%20filename%3Drun.sh&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-03-29 12:08:02--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/292584141/ba8c6091-57ba-40fd-aa44-da9906d56075?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240329%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240329T120802Z&X-Amz-Expires=300&X-Amz-Signature=3b8e513ffc0896f923a50c511ad3b2f9a41ca97997daf7cc7af03e257783bc3f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=292584141&response-content-disposition=attachment%3B%20filename%3Drun.sh&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 270 [application/octet-stream]\n",
            "Saving to: ‘run.sh’\n",
            "\n",
            "run.sh              100%[===================>]     270  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-29 12:08:03 (9.59 MB/s) - ‘run.sh’ saved [270/270]\n",
            "\n",
            "ok\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title 0. Installing requirements (click \"show code\" for details)\n",
        "!echo 'installing text extraction scripts (GhostScript)' 1>&2\n",
        "!echo '================================================' 1>&2\n",
        "!if apt-get install ghostscript >& tmp.log; \\\n",
        " then echo done 1>&2; \\\n",
        " else cat tmp.log 1>&2; \\\n",
        " fi;\n",
        "!rm tmp.log;\n",
        "!echo 1>&2;\n",
        "\n",
        "!echo 'installing parser (spaCy)' 1>&2\n",
        "!echo '=========================' 1>&2\n",
        "!if  pip install spacy_conll spacy_stanza >& tmp.log; \\\n",
        " then echo done 1>&2;\\\n",
        " else cat tmp.log 1>&2;\\\n",
        " fi;\\\n",
        " rm tmp.log\n",
        "!echo 1>&2\n",
        "\n",
        "!echo 'installing FINTAN' 1>&2\n",
        "!echo '=================' 1>&2\n",
        "!if [ ! -e /fintan ]; then mkdir /fintan; \\\n",
        " cd /fintan;\\\n",
        " wget -nc https://github.com/acoli-repo/fintan-backend/releases/download/fintan-backend-release-v1.0.0/fintan.jar;\\\n",
        " wget -nc https://github.com/acoli-repo/fintan-backend/releases/download/fintan-backend-release-v1.0.0/run.sh;\\\n",
        " chmod u+x /fintan/run.sh;\\\n",
        " fi;\n",
        "!if [ -x /fintan/run.sh ]; \\\n",
        " then echo ok 1>&2;\\\n",
        " else echo /fintan/run.sh not executable ! 1>&2;\\\n",
        " fi;\n",
        "!echo 1>&2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaW0DEsYDwat",
        "outputId": "bad08a89-a4be-4274-b6f9-fb7e905a927f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building sample corpus\n",
            "======================\n",
            "https://aclanthology.org/2011.tal-3.10.pdf > sample1.pdf\n",
            "\n",
            "retrieved PDFs\n",
            "==============\n",
            "samples/pdf/sample1.pdf\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Retrieve sample document(s) into `samples/pdf` folder (click \"show code\" for details).\n",
        "!echo  building sample corpus 1>&2\n",
        "!echo '======================' 1>&2\n",
        "\n",
        "! if [ -e samples/ ]; then rm -rf samples/; fi;\n",
        "!mkdir -p samples/pdf;\n",
        "!url2files=\"https://aclanthology.org/2011.tal-3.10.pdf=sample1.pdf\";\\\n",
        " for url2file in $url2files; do \\\n",
        "  url=`echo $url2file | sed s/'=.*'//`;\\\n",
        "  file=`basename $url2file | sed s/'.*='//`;\\\n",
        "  echo $url '>' $file 1>&2;\\\n",
        "  wget -nc $url -O samples/pdf/$file >&/dev/null; \\\n",
        " done;\n",
        "!echo 1>&2\n",
        "\n",
        "!echo  retrieved PDFs 1>&2\n",
        "!echo '==============' 1>&2\n",
        "\n",
        "!find samples/ | grep '\\.pdf' 1>&2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ5TMK_uLo3a"
      },
      "source": [
        "  > Note: To process our own files to the script above, just add URLs and (optionally) file names to the `url2file` variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5lDXciaFaP8",
        "outputId": "1f86185e-84c0-427c-e3a9-8fad9b0e0984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perform text extraction\n",
            "=======================\n",
            "samples/pdf/sample1.pdf > samples/txt/sample1.pdf.txt .."
          ]
        }
      ],
      "source": [
        "# @title 2. Perform text extraction over all PDF files in `samples/pdf` (click \"show code\" for details).\n",
        "!echo  perform text extraction 1>&2\n",
        "!echo '=======================' 1>&2\n",
        "!if [ ! -e samples/txt ]; then mkdir -p samples/txt; fi;\n",
        "!for file in samples/pdf/*.pdf; do \\\n",
        "  tgt=samples/txt/`basename $file`.txt;\\\n",
        "  echo -n $file '>' $tgt' ..' 1>&2;\\\n",
        "  ps2ascii $file > $tgt;\\\n",
        "  if [ -s $file ]; \\\n",
        "  then echo '. ok' 1>&2;\\\n",
        "  else echo '. failed' 1>62;\\\n",
        "  fi;\\\n",
        " done;\n",
        "!echo 1>&2\n",
        "\n",
        "!echo  resulting text files 1>&2;\n",
        "!echo '====================' 1>&2;\n",
        "!find samples/txt | grep '\\.txt' 1>&2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z6pWeoTOHVf"
      },
      "outputs": [],
      "source": [
        "# @title 3. Parse all text files using spaCy (click \"show code\" for details)\n",
        "# code adapted from https://spacy.io/universe/project/spacy-conll\n",
        "\n",
        "from spacy_conll import init_parser\n",
        "import os,re\n",
        "\n",
        "# Initialise English parser, already including the ConllFormatter as a pipeline component.\n",
        "# Indicate that we want to get the CoNLL headers in the string output.\n",
        "# `use_gpu` and `verbose` are specific to stanza. These keywords arguments are passed onto their Pipeline() initialisation\n",
        "print(\"initializing parser\")\n",
        "print(\"===================\")\n",
        "nlp = init_parser(\"en\",\n",
        "                  \"stanza\",\n",
        "                  parser_opts={\"use_gpu\": True, \"verbose\": False},\n",
        "                  include_headers=True)\n",
        "print(\"done\\n\")\n",
        "\n",
        "txt_dir=\"./samples/txt\"\n",
        "out_dir=\"./samples/conll\"\n",
        "if not os.path.exists(out_dir):\n",
        "  os.makedirs(out_dir)\n",
        "\n",
        "print(\"parsing\")\n",
        "print(\"=======\")\n",
        "\n",
        "for file in os.listdir(txt_dir):\n",
        "  file=os.path.join(txt_dir,file)\n",
        "  tgt=os.path.join(out_dir,re.sub(r\".*/\",\"\",file)+\".conllu\")\n",
        "  print(f\"{file} > {tgt}\", end=\" ..\")\n",
        "  with(open(file,\"rt\",errors=\"ignore\")) as input:\n",
        "    txt=input.read()\n",
        "    with open(tgt,\"wt\",errors=\"ignore\") as output:\n",
        "      txt=re.sub(r\"<[^>]*>\",\"\",txt)\n",
        "      txt=re.sub(r\"([.!?])\\s*\\n\",\"\\1<br>\",txt)\n",
        "      txt=re.sub(r\"\\n([(\\\"]*[A-Z])\",r\"<br>\\1\", txt)\n",
        "      txt=re.sub(r\"\\n\\s*\\n\",\"<br>\",txt)\n",
        "      txt=re.sub(r\"([a-z])[\\-]\\s*\\n\\s*\",\"\\1\",txt)\n",
        "      txt=\" \".join(txt.split()).strip()\n",
        "      txt=\"\\n\\n\".join(txt.split(\"<br>\"))\n",
        "      txt=re.sub(r\"([aeiouyAEIOUY][^\\s]*[!?.][\\\")]*)\\s+([\\\"(]*[A-Z])\",\"\\1\\n\\2\",txt)\n",
        "      for sent in txt.split(\"\\n\"):\n",
        "        sent=sent.strip()\n",
        "        if len(sent)==0:\n",
        "          output.write(sent)\n",
        "        else:\n",
        "          parsed=nlp(sent)\n",
        "          output.write(parsed._.conll_str+\"\\n\")\n",
        "          output.flush()\n",
        "    print(\". ok\")\n",
        "print()\n",
        "\n",
        "!echo  resulting CoNLL files 1>&2;\n",
        "!echo '=====================' 1>&2;\n",
        "!find samples/ | grep '\\.conll' 1>&2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lzfloorOSn6"
      },
      "source": [
        "> *Note*: Since Jun 2022, the [spaCy parser](https://spacy.io/) comes pre-installed with Google Colaboratory. In case this would change in the future, see https://spacy.io/ for installation instructions. Alternatively, spaCy can be replaced by any other UD parser.\n",
        "\n",
        "> *Note*: At present, the parser is remarkably slow, but this is because the input isn't properly pre-processed. With an additional sentence splitting (and merging) step, it should be significantly faster. To be added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXyZLwuGGF1X"
      },
      "outputs": [],
      "source": [
        "# @title 4. Generating RDF Data\n",
        "\n",
        "!if [ ! -e samples/ttl ]; then mkdir samples/ttl; fi\n",
        "\n",
        "!echo 'Configure FINTAN for CoNLL-RDF' 1>&2\n",
        "!echo '==============================' 1>&2\n",
        "\n",
        "import json\n",
        "ttl_dir=\"samples/ttl\"\n",
        "for file in os.listdir(out_dir):\n",
        "  tgt=os.path.join(ttl_dir,file+\".ttl\")\n",
        "  conf=os.path.join(\".\",\"conf_\"+file+\".ttl.json\")\n",
        "  file=os.path.join(out_dir,file)\n",
        "  print(f\"{file} > {conf}\",end=\" ..\")\n",
        "  with open(conf,\"wt\") as output:\n",
        "    json.dump(\n",
        "      {\"input\" : file, \"output\" : tgt, \"pipeline\" : [\n",
        "        { \"class\" : \"CoNLLStreamExtractor\",\n",
        "          \"baseURI\" : file+\"#\", \"columns\" : [\"ID\", \"WORD\", \"LEMMA\", \"UPOS\", \"POS\", \"FEAT\", \"HEAD\", \"EDGE\", \"DEPS\", \"MISC\"] },\n",
        "        { \"class\" : \"CoNLLRDFFormatter\",\n",
        "          \"modules\" : [ {\"mode\":\"RDF\", \"columns\": [\"ID\", \"WORD\", \"LEMMA\", \"UPOS\", \"POS\", \"FEAT\", \"HEAD\", \"EDGE\", \"DEPS\", \"MISC\"]} ]\n",
        "        } ] },\n",
        "      output)\n",
        "  print(f\". ok\")\n",
        "!echo 1>&2\n",
        "\n",
        "!echo 'Convert to CoNLL-RDF' 1>&2\n",
        "!echo '====================' 1>&2\n",
        "!for conf in conf*ttl.json; do \\\n",
        "  echo $conf 1>&2;\\\n",
        "  /fintan/run.sh -c $conf 2>/dev/null;\\\n",
        "  rm $conf;\\\n",
        " done;\n",
        "!echo 1>&2\n",
        "\n",
        "!echo 'Resulting RDF/TTL files' 1>&2\n",
        "!echo '=======================' 1>&2\n",
        "!find samples/ttl | grep '\\.ttl' 1>&2\n",
        "!echo 1>&2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBMB6Mgo4XI"
      },
      "source": [
        "> *Note*: FINTAN is not actually meant to convert block data. Rather, it is optimized for on-the-fly stream processing over large data quantities, and specifically, to enable data transformation and enrichment with SPARQL without loading the entire dataset into memory. In general, this will be significantly faster than SPARQL querying/updates over a database. Try the [command-line interface](https://github.com/acoli-repo/conll-rdf) to unleash its full potential. A much more effective implementation would be to feed spaCy output directly into FINTAN.\n",
        "\n",
        "> *Note*: In addition to parsing CoNLL data, CoNLL-RDF can also generate different CoNLL formats. In fact, it is designed to complement standard NLP pipelines (using CoNLL or similar exchange formats) with SPARQL rewriting rules. In the subsequent [FINTAN platform](https://github.com/acoli-repo/fintan), the range of input and output formats has been significantly extended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JucK49FLeMdx"
      },
      "outputs": [],
      "source": [
        "# @title 5. Download the resulting annotations in a zip archive\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"sample\",\"zip\",ttl_dir) # first argument is file name without file extension, second argment is file extension\n",
        "files.download(\"sample.zip\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}